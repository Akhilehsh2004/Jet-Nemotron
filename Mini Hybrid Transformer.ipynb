{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "969ad22c-2262-4583-b96d-ab7344f505fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([8, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -------------------------\n",
    "# Scaled Dot-Product Attention (standard)\n",
    "# -------------------------\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k):\n",
    "        super().__init__()\n",
    "        self.scale = d_k ** -0.5\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output, attn_probs\n",
    "\n",
    "# -------------------------\n",
    "# Linear Attention (efficient for long seq)\n",
    "# -------------------------\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # Î¦(x) = elu(x)+1 (positive kernel feature map)\n",
    "        Q = F.elu(Q) + 1\n",
    "        K = F.elu(K) + 1\n",
    "\n",
    "        KV = torch.einsum('...nd,...ne->...de', K, V)  # (d, e)\n",
    "        Z = 1.0 / torch.einsum('...nd,...d->...n', Q, K.sum(dim=-2) + 1e-6)\n",
    "        output = torch.einsum('...nd,...de,...n->...ne', Q, KV, Z)\n",
    "        return output\n",
    "\n",
    "# -------------------------\n",
    "# Hybrid Multi-Head Attention\n",
    "# -------------------------\n",
    "class HybridAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, seq_threshold=128):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.seq_threshold = seq_threshold\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.full_attn = ScaledDotProductAttention(self.d_k)\n",
    "        self.linear_attn = LinearAttention()\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, N, D = x.shape\n",
    "\n",
    "        Q = self.W_q(x).view(B, N, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(B, N, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(B, N, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        if N <= self.seq_threshold:\n",
    "            out, _ = self.full_attn(Q, K, V, mask)\n",
    "        else:\n",
    "            out = self.linear_attn(Q, K, V)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(B, N, D)\n",
    "        return self.fc_out(out)\n",
    "\n",
    "# -------------------------\n",
    "# Transformer Encoder Block\n",
    "# -------------------------\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, ff_hidden, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = HybridAttention(d_model, n_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden, d_model)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out = self.attn(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))\n",
    "        return x\n",
    "\n",
    "# -------------------------\n",
    "# Mini Hybrid Transformer (for classification)\n",
    "# -------------------------\n",
    "class MiniHybridTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=128, n_heads=4, ff_hidden=256, num_classes=10, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, ff_hidden) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        x = x.mean(dim=1)  # Pooling\n",
    "        return self.fc_out(x)\n",
    "\n",
    "# -------------------------\n",
    "# Test Run (CPU)\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = MiniHybridTransformer(input_dim=64, num_classes=5).to(device)\n",
    "\n",
    "    dummy_input = torch.randn(8, 150, 64).to(device)  # (batch, seq_len, features)\n",
    "    out = model(dummy_input)\n",
    "    print(\"Output shape:\", out.shape)  # Expected: (8, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6425d62-6e92-4b4d-a0fe-5910c4dc382a",
   "metadata": {},
   "source": [
    "\n",
    "## **What the code does step by step**:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Imports**\n",
    "\n",
    "It loads the required Python libraries:\n",
    "\n",
    "* `torch` â†’ for deep learning.\n",
    "* `torch.nn` â†’ for building layers (like Linear, LayerNorm, Dropout).\n",
    "* `torch.nn.functional` â†’ for activation functions (like softmax, relu).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Scaled Dot-Product Attention (standard attention)**\n",
    "\n",
    "This is the **Full Attention** part:\n",
    "\n",
    "* Takes **queries (Q), keys (K), and values (V)**.\n",
    "* Computes attention scores by `Q*Káµ€ / sqrt(d)`.\n",
    "* Applies **softmax** to convert scores into probabilities.\n",
    "* Multiplies probabilities with **V** to get the final weighted output.\n",
    "\n",
    "This is the **classic transformer attention**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Linear Attention (efficient attention)**\n",
    "\n",
    "This is the **fast version**:\n",
    "\n",
    "* Instead of computing the full `Q*Káµ€` matrix (which is slow for large inputs),\n",
    "  it uses an approximation where softmax is applied differently.\n",
    "* Idea: compute `(QÂ·(softmax(K)áµ€Â·V))` instead of `(softmax(QÂ·Káµ€)Â·V)`.\n",
    "* Saves a **lot of memory and time** when sequence length is huge.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **MiniHybridAttention (hybrid block)**\n",
    "\n",
    "* This block combines **both Full Attention and Linear Attention**.\n",
    "* It runs both attentions separately:\n",
    "\n",
    "  * `out_full` = result from normal attention.\n",
    "  * `out_linear` = result from linear attention.\n",
    "* Then it **averages them** â†’ `(out_full + out_linear) / 2`.\n",
    "  (You can also add a trainable weight to decide how much to trust each one.)\n",
    "* This makes it a **hybrid attention mechanism**.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **MiniHybridTransformer (the model)**\n",
    "\n",
    "* **Input layer**: Embeds the input vector using a linear layer.\n",
    "* **Attention block**: Uses `MiniHybridAttention`.\n",
    "* **Feed-forward block**: A small neural network with two linear layers and ReLU activation.\n",
    "* **LayerNorm**: Helps stabilize training.\n",
    "* **Dropout**: Prevents overfitting.\n",
    "* **Output layer**: Converts hidden state back to vocab size (for classification or prediction).\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Running the Model (example forward pass)**\n",
    "\n",
    "* Creates some **dummy input data** (batch of 2 sequences, each of length 10, vocab size 50).\n",
    "* Feeds it into the model.\n",
    "* Prints the output shape â†’ `(batch_size, seq_len, vocab_size)`.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **In short:**\n",
    "This code builds a **small transformer-like model** that **mixes full attention and linear attention** into one **hybrid mechanism**, making it faster and more memory-efficient while still keeping the accuracy of normal attention.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e46a7e0-f63b-4d3f-9e53-72f95c04d977",
   "metadata": {},
   "source": [
    "**Line by line** through the **Mini Hybrid Transformer** code \n",
    "---\n",
    "\n",
    "## ðŸ“Œ Code Recap (Mini Hybrid Transformer for CPU)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Hybrid Attention: combines Full Attention (exact) and Linear Attention (approximate)\n",
    "class HybridAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, linear_ratio=0.5):\n",
    "        super(HybridAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.linear_ratio = linear_ratio\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, D = x.shape  # Batch size, Sequence length, Embedding size\n",
    "\n",
    "        # Project input into Q, K, V\n",
    "        Q = self.q_linear(x).view(B, N, self.n_heads, D // self.n_heads).transpose(1, 2)\n",
    "        K = self.k_linear(x).view(B, N, self.n_heads, D // self.n_heads).transpose(1, 2)\n",
    "        V = self.v_linear(x).view(B, N, self.n_heads, D // self.n_heads).transpose(1, 2)\n",
    "\n",
    "        # Split heads into full and linear\n",
    "        split = int(self.n_heads * self.linear_ratio)\n",
    "        Q_full, K_full, V_full = Q[:, :split], K[:, :split], V[:, :split]\n",
    "        Q_lin, K_lin, V_lin = Q[:, split:], K[:, split:], V[:, split:]\n",
    "\n",
    "        # Full attention\n",
    "        attn_scores = torch.matmul(Q_full, K_full.transpose(-2, -1)) / (D ** 0.5)\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        full_out = torch.matmul(attn_probs, V_full)\n",
    "\n",
    "        # Linear attention (kernel trick: relu(Q) * (relu(K).T @ V))\n",
    "        K_lin_relu = F.relu(K_lin)\n",
    "        Q_lin_relu = F.relu(Q_lin)\n",
    "        KV = torch.matmul(K_lin_relu.transpose(-2, -1), V_lin)\n",
    "        lin_out = torch.matmul(Q_lin_relu, KV)\n",
    "\n",
    "        # Concatenate outputs\n",
    "        out = torch.cat([full_out, lin_out], dim=1).transpose(1, 2).contiguous().view(B, N, D)\n",
    "        return self.out(out)\n",
    "\n",
    "# Mini Transformer Block with Hybrid Attention\n",
    "class MiniHybridTransformer(nn.Module):\n",
    "    def __init__(self, d_model=64, n_heads=4, linear_ratio=0.5, dim_ff=128):\n",
    "        super(MiniHybridTransformer, self).__init__()\n",
    "        self.attn = HybridAttention(d_model, n_heads, linear_ratio)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_ff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))  # Residual connection\n",
    "        x = x + self.ff(self.norm2(x))    # Residual connection\n",
    "        return x\n",
    "\n",
    "# Test model on CPU\n",
    "if __name__ == \"__main__\":\n",
    "    model = MiniHybridTransformer(d_model=64, n_heads=4, linear_ratio=0.5).cpu()\n",
    "    x = torch.randn(2, 10, 64)  # (batch=2, seq_len=10, embedding=64)\n",
    "    y = model(x)\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Output shape:\", y.shape)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ”Ž Line-by-Line Explanation\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Import libraries\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "```\n",
    "\n",
    "* `torch` â†’ Core PyTorch library.\n",
    "* `nn` â†’ Used to build layers like Linear, ReLU, LayerNorm.\n",
    "* `F` â†’ Contains useful functions (softmax, relu, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Define **Hybrid Attention**\n",
    "\n",
    "```python\n",
    "class HybridAttention(nn.Module):\n",
    "```\n",
    "\n",
    "* Creates a **new type of Attention layer** that mixes **Full Attention** and **Linear Attention**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Initialize constructor\n",
    "\n",
    "```python\n",
    "def __init__(self, d_model, n_heads, linear_ratio=0.5):\n",
    "    super(HybridAttention, self).__init__()\n",
    "```\n",
    "\n",
    "* `d_model`: size of embeddings (example: 64).\n",
    "* `n_heads`: number of attention heads (example: 4).\n",
    "* `linear_ratio`: how many heads use **linear attention** (rest use full attention).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Define weight matrices\n",
    "\n",
    "```python\n",
    "self.q_linear = nn.Linear(d_model, d_model)\n",
    "self.k_linear = nn.Linear(d_model, d_model)\n",
    "self.v_linear = nn.Linear(d_model, d_model)\n",
    "self.out = nn.Linear(d_model, d_model)\n",
    "```\n",
    "\n",
    "* These are the **Q (Query), K (Key), V (Value)** projection layers.\n",
    "* `self.out` â†’ final layer to recombine head outputs.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Forward method (main computation)\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    B, N, D = x.shape\n",
    "```\n",
    "\n",
    "* `B` = batch size (number of samples at once).\n",
    "* `N` = sequence length (words/tokens).\n",
    "* `D` = embedding size (features per token).\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Project inputs into Q, K, V\n",
    "\n",
    "```python\n",
    "Q = self.q_linear(x).view(B, N, self.n_heads, D // self.n_heads).transpose(1, 2)\n",
    "K = self.k_linear(x).view(B, N, self.n_heads, D // self.n_heads).transpose(1, 2)\n",
    "V = self.v_linear(x).view(B, N, self.n_heads, D // self.n_heads).transpose(1, 2)\n",
    "```\n",
    "\n",
    "* Convert input into Q, K, V matrices.\n",
    "* Split into multiple **heads** (`n_heads`).\n",
    "* `transpose(1, 2)` â†’ puts `n_heads` dimension in front.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Split heads into **full** and **linear**\n",
    "\n",
    "```python\n",
    "split = int(self.n_heads * self.linear_ratio)\n",
    "Q_full, K_full, V_full = Q[:, :split], K[:, :split], V[:, :split]\n",
    "Q_lin, K_lin, V_lin = Q[:, split:], K[:, split:], V[:, split:]\n",
    "```\n",
    "\n",
    "* First few heads (`split`) â†’ use **Full Attention**.\n",
    "* Remaining heads â†’ use **Linear Attention**.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Full Attention\n",
    "\n",
    "```python\n",
    "attn_scores = torch.matmul(Q_full, K_full.transpose(-2, -1)) / (D ** 0.5)\n",
    "attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "full_out = torch.matmul(attn_probs, V_full)\n",
    "```\n",
    "\n",
    "* Calculate similarity between queries & keys.\n",
    "* Normalize with **softmax**.\n",
    "* Multiply with values â†’ gives **weighted sum** (context).\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Linear Attention\n",
    "\n",
    "```python\n",
    "K_lin_relu = F.relu(K_lin)\n",
    "Q_lin_relu = F.relu(Q_lin)\n",
    "KV = torch.matmul(K_lin_relu.transpose(-2, -1), V_lin)\n",
    "lin_out = torch.matmul(Q_lin_relu, KV)\n",
    "```\n",
    "\n",
    "* Uses **ReLU kernel trick** for efficiency.\n",
    "* Instead of `softmax(QKáµ€)`, it computes `(ReLU(Q) * (ReLU(K)áµ€ @ V))`.\n",
    "* Much faster for long sequences.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Concatenate results\n",
    "\n",
    "```python\n",
    "out = torch.cat([full_out, lin_out], dim=1).transpose(1, 2).contiguous().view(B, N, D)\n",
    "return self.out(out)\n",
    "```\n",
    "\n",
    "* Combines **full + linear attention outputs**.\n",
    "* Reorders & reshapes to original format `(B, N, D)`.\n",
    "* Passes through final `Linear` layer.\n",
    "\n",
    "---\n",
    "\n",
    "### 11. Define **Transformer Block**\n",
    "\n",
    "```python\n",
    "class MiniHybridTransformer(nn.Module):\n",
    "```\n",
    "\n",
    "* A block = **Attention + FeedForward + Residuals + Norm**.\n",
    "\n",
    "---\n",
    "\n",
    "### 12. Constructor\n",
    "\n",
    "```python\n",
    "def __init__(self, d_model=64, n_heads=4, linear_ratio=0.5, dim_ff=128):\n",
    "```\n",
    "\n",
    "* `d_model=64` â†’ embedding size.\n",
    "* `n_heads=4` â†’ multi-head attention.\n",
    "* `dim_ff=128` â†’ hidden size for feed-forward network.\n",
    "\n",
    "---\n",
    "\n",
    "### 13. Define layers\n",
    "\n",
    "```python\n",
    "self.attn = HybridAttention(d_model, n_heads, linear_ratio)\n",
    "self.ff = nn.Sequential(\n",
    "    nn.Linear(d_model, dim_ff),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(dim_ff, d_model)\n",
    ")\n",
    "self.norm1 = nn.LayerNorm(d_model)\n",
    "self.norm2 = nn.LayerNorm(d_model)\n",
    "```\n",
    "\n",
    "* `self.attn`: Hybrid attention.\n",
    "* `self.ff`: Feed-forward network.\n",
    "* `self.norm1` & `self.norm2`: stabilize training.\n",
    "\n",
    "---\n",
    "\n",
    "### 14. Forward pass\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    x = x + self.attn(self.norm1(x))  # Residual connection\n",
    "    x = x + self.ff(self.norm2(x))    # Residual connection\n",
    "    return x\n",
    "```\n",
    "\n",
    "* Apply **LayerNorm â†’ Attention â†’ Residual**.\n",
    "* Apply **LayerNorm â†’ FeedForward â†’ Residual**.\n",
    "* Return the transformed output.\n",
    "\n",
    "---\n",
    "\n",
    "### 15. Testing model on CPU\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    model = MiniHybridTransformer(d_model=64, n_heads=4, linear_ratio=0.5).cpu()\n",
    "    x = torch.randn(2, 10, 64)  # (batch=2, seq_len=10, embedding=64)\n",
    "    y = model(x)\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Output shape:\", y.shape)\n",
    "```\n",
    "\n",
    "* Creates model.\n",
    "* Input: `2` batches, sequence length `10`, embedding size `64`.\n",
    "* Prints input & output shapes.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **In short:**\n",
    "This is a **mini Transformer block** that mixes **Full Attention (accurate but slow)** with **Linear Attention (fast but approximate)** to balance **speed and accuracy**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e422f384-8929-4ce6-b816-112e056a82b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ðŸ“˜ **Interview Questions on Full Attention vs Linear Attentionl break this into **three levels: Easy, Moderate, Hard**â€”so you can handle** **MCA + CSE-level interviews** fully prepared.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ”¹ Easy Level Questions (Basics)\n",
    "\n",
    "1. **Q:** What is the main purpose of this code?\n",
    "   **A:** To implement and test a hybrid transformer model combining full attention and linear attention on CPU.\n",
    "\n",
    "2. **Q:** Which library is primarily used in this code?\n",
    "   **A:** PyTorch (`torch`).\n",
    "\n",
    "3. **Q:** What does the `MiniHybridTransformer` class represent?\n",
    "   **A:** A custom transformer model that mixes full and linear attention.\n",
    "\n",
    "4. **Q:** What is the function of `nn.Embedding` in this code?\n",
    "   **A:** Converts token indices into dense vector representations.\n",
    "\n",
    "5. **Q:** What is the role of `nn.TransformerEncoder`?\n",
    "   **A:** It stacks multiple transformer encoder layers for sequence modeling.\n",
    "\n",
    "6. **Q:** What does `nn.TransformerEncoderLayer` do?\n",
    "   **A:** Defines a single transformer encoder block with attention + feedforward layers.\n",
    "\n",
    "7. **Q:** What is linear attention?\n",
    "   **A:** An approximation of standard attention that reduces computation from quadratic to linear in sequence length.\n",
    "\n",
    "8. **Q:** Why do we use `nn.Linear` at the output?\n",
    "   **A:** To map hidden states to vocabulary size for prediction.\n",
    "\n",
    "9. **Q:** Why is ReLU used in linear attention?\n",
    "   **A:** To ensure positive features and stabilize attention approximation.\n",
    "\n",
    "10. **Q:** What does `batch_size=2` mean?\n",
    "    **A:** Two input sequences are processed at the same time.\n",
    "\n",
    "11. **Q:** Why is `torch.randn` used in linear attention?\n",
    "    **A:** To generate random projection vectors for approximation.\n",
    "\n",
    "12. **Q:** Why do we set `dtype=torch.float32`?\n",
    "    **A:** To define the precision of computations.\n",
    "\n",
    "13. **Q:** What is the function of the `forward` method in PyTorch models?\n",
    "    **A:** Defines how input data flows through the model.\n",
    "\n",
    "14. **Q:** Why do we call `.to(device)`?\n",
    "    **A:** To move tensors and models to CPU or GPU.\n",
    "\n",
    "15. **Q:** What is the role of `seq_len` in this code?\n",
    "    **A:** The length of each input sequence (10 tokens here).\n",
    "\n",
    "16. **Q:** Why are vocab\\_size and embed\\_dim important?\n",
    "    **A:** They define the input token space and the size of embeddings.\n",
    "\n",
    "17. **Q:** What is the purpose of `torch.randint`?\n",
    "    **A:** To generate random integer token IDs for testing.\n",
    "\n",
    "18. **Q:** What does `src.transpose(0, 1)` do?\n",
    "    **A:** Changes shape to match transformer input format: `(seq_len, batch, embed_dim)`.\n",
    "\n",
    "19. **Q:** Why does the transformer need `(seq_len, batch, embed_dim)`?\n",
    "    **A:** It expects time-first input format.\n",
    "\n",
    "20. **Q:** What does the hybrid model output shape `(2, 10, 50)` mean?\n",
    "    **A:** Batch size = 2, sequence length = 10, vocab predictions = 50.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ”¹ Moderate Level Questions (Deeper Understanding)\n",
    "\n",
    "21. **Q:** What is the difference between full attention and linear attention?\n",
    "    **A:** Full attention computes all pairwise token interactions (O(nÂ²)), while linear attention approximates them to O(n).\n",
    "\n",
    "22. **Q:** Why would we mix both attentions in one model?\n",
    "    **A:** To balance accuracy (from full attention) and efficiency (from linear attention).\n",
    "\n",
    "23. **Q:** Why do we use random projection in linear attention?\n",
    "    **A:** To reduce the dimensionality of attention computations.\n",
    "\n",
    "24. **Q:** How does scaling by `1/sqrt(d_k)` help in attention?\n",
    "    **A:** Prevents large dot-product values, stabilizing gradients.\n",
    "\n",
    "25. **Q:** Why does the code define `MiniHybridTransformer` instead of using only PyTorchâ€™s built-in transformer?\n",
    "    **A:** To add the linear attention mechanism alongside the default full attention.\n",
    "\n",
    "26. **Q:** Why is `ReLU(Q @ projection)` used instead of softmax?\n",
    "    **A:** It avoids quadratic normalization cost, making it linear-time.\n",
    "\n",
    "27. **Q:** What would happen if we increased `seq_len` from 10 to 1000?\n",
    "    **A:** Full attention would slow down (O(nÂ²)), but linear attention would scale better.\n",
    "\n",
    "28. **Q:** What is the role of the `output_layer`?\n",
    "    **A:** Projects hidden states to logits over vocabulary for prediction.\n",
    "\n",
    "29. **Q:** What happens if we remove the `output_layer`?\n",
    "    **A:** The model wonâ€™t produce class probabilities, just embeddings.\n",
    "\n",
    "30. **Q:** Why do we use `torch.matmul` in attention computation?\n",
    "    **A:** To compute dot-products efficiently between queries, keys, and values.\n",
    "\n",
    "31. **Q:** How is memory usage different in linear vs. full attention?\n",
    "    **A:** Full attention stores an nÃ—n matrix; linear stores smaller projected representations.\n",
    "\n",
    "32. **Q:** Why is `nn.TransformerEncoderLayer` stacked into `nn.TransformerEncoder`?\n",
    "    **A:** To build deeper models with multiple layers of attention.\n",
    "\n",
    "33. **Q:** Can this hybrid transformer be used for NLP tasks?\n",
    "    **A:** Yes, for tasks like text classification, translation, summarization.\n",
    "\n",
    "34. **Q:** Why does `vocab_size=50` not match real-world NLP vocabularies?\n",
    "    **A:** This is just a toy example; real models use vocab sizes in the tens of thousands.\n",
    "\n",
    "35. **Q:** Why did we not train the model here?\n",
    "    **A:** The code is only a forward pass demo, not training.\n",
    "\n",
    "36. **Q:** Why might linear attention lose accuracy compared to full attention?\n",
    "    **A:** Because approximation reduces exact token-to-token interactions.\n",
    "\n",
    "37. **Q:** Why do we use batch processing in transformers?\n",
    "    **A:** To process multiple sequences simultaneously for efficiency.\n",
    "\n",
    "38. **Q:** Why is normalization important in transformers?\n",
    "    **A:** It stabilizes training by keeping activations in a reasonable range.\n",
    "\n",
    "39. **Q:** Can we extend this to a decoder model?\n",
    "    **A:** Yes, by adding transformer decoder layers.\n",
    "\n",
    "40. **Q:** Why might this model perform differently on CPU vs GPU?\n",
    "    **A:** GPUs are optimized for matrix multiplications, while CPUs may be slower for large inputs.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ”¹ Hard Level Questions (Advanced / Research)\n",
    "\n",
    "41. **Q:** How does linear attention achieve O(n) complexity?\n",
    "    **A:** By using kernel tricks and projections to avoid computing the full nÃ—n attention matrix.\n",
    "\n",
    "42. **Q:** Whatâ€™s the tradeoff between accuracy and speed in hybrid transformers?\n",
    "    **A:** Full attention is more accurate but slower; linear attention is faster but may lose precision.\n",
    "\n",
    "43. **Q:** Why are random projections effective in approximating attention?\n",
    "    **A:** They preserve distances probabilistically (Johnson-Lindenstrauss lemma).\n",
    "\n",
    "44. **Q:** How would you modify this code to train on a text dataset like IMDB reviews?\n",
    "    **A:** Replace random tokens with actual dataset inputs and add a loss function + optimizer.\n",
    "\n",
    "45. **Q:** Why is position encoding not included here?\n",
    "    **A:** For simplicity, but real transformers need positional information to capture order.\n",
    "\n",
    "46. **Q:** How could we add sinusoidal positional encodings here?\n",
    "    **A:** By adding a `PositionalEncoding` module before embeddings.\n",
    "\n",
    "47. **Q:** Why is multi-head attention not explicitly implemented in the linear attention function?\n",
    "    **A:** For simplicity; real-world implementations extend linear attention to multi-head form.\n",
    "\n",
    "48. **Q:** What is the effect of `ReLU` in linear attention normalization?\n",
    "    **A:** It ensures non-negativity, allowing normalization like a probability distribution.\n",
    "\n",
    "49. **Q:** Can linear attention replace full attention completely?\n",
    "    **A:** Sometimes yes, but accuracy may drop in tasks needing long-range dependencies.\n",
    "\n",
    "50. **Q:** How would you integrate memory-efficient attention mechanisms like FlashAttention into this model?\n",
    "    **A:** Replace the attention computation with FlashAttention kernels.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fingerprint)",
   "language": "python",
   "name": "fingerprint-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
